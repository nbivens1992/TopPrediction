{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.layers import Normalization, Lambda\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_global_view():\n",
    "    inputs = layers.Input(shape=(None, 3))  # Assuming the logs are 1D sequences\n",
    "    \n",
    "    x = layers.Masking(mask_value=-99)(inputs)  # Using -99 as the mask value\n",
    "\n",
    "    # Encoding layer 1\n",
    "    conv1 = layers.Conv1D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    conv1 = layers.BatchNormalization()(conv1)\n",
    "    conv1 = layers.Dropout(0.5)(conv1)\n",
    "    pool1 = layers.MaxPooling1D()(conv1)\n",
    "\n",
    "    # Encoding layer 2\n",
    "    conv2 = layers.Conv1D(128, 3, activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = layers.BatchNormalization()(conv2)\n",
    "    conv2 = layers.Dropout(0.5)(conv2)\n",
    "    pool2 = layers.MaxPooling1D()(conv2)\n",
    "\n",
    "    # Middle layer\n",
    "    conv_middle = layers.Conv1D(256, 3, activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv_middle = layers.BatchNormalization()(conv_middle)\n",
    "    conv_middle = layers.Dropout(0.5)(conv_middle)\n",
    "\n",
    "    # Decoding layer 1\n",
    "    up1 = layers.UpSampling1D()(conv_middle)\n",
    "    \n",
    "    # # Ensure sizes match before concatenating\n",
    "    # cropped_conv2 = layers.Cropping1D(cropping=((1,0)))(conv2)\n",
    "    merge1 = layers.concatenate([conv2, up1])\n",
    "    \n",
    "    decode1 = layers.Conv1D(128, 3, activation=\"relu\", padding=\"same\")(merge1)\n",
    "    decode1 = layers.BatchNormalization()(decode1)\n",
    "    decode1 = layers.Dropout(0.5)(decode1)\n",
    "\n",
    "    # Decoding layer 2\n",
    "    up2 = layers.UpSampling1D()(decode1)\n",
    "    \n",
    "    # Ensure sizes match before concatenating for the second merge\n",
    "    # cropped_conv1 = layers.Cropping1D(cropping=((2,0)))(conv1)  # Adjusted cropping\n",
    "    merge2 = layers.concatenate([conv1, up2])\n",
    "\n",
    "    decode2 = layers.Conv1D(64, 3, activation=\"relu\", padding=\"same\")(merge2)\n",
    "    decode2 = layers.BatchNormalization()(decode2)\n",
    "    decode2 = layers.Dropout(0.5)(decode2)\n",
    "\n",
    "    return models.Model(inputs, decode2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local View\n",
    "def create_local_view():\n",
    "    inputs = layers.Input(shape=(None, 3))\n",
    "    x = layers.Masking(mask_value=-99)(inputs)  # Using -99 as the mask values\n",
    "    \n",
    "    # Inception layers with dilated convolutions\n",
    "    conv1 = layers.Conv1D(32, 1, padding=\"same\", dilation_rate=1)(x)\n",
    "    conv3 = layers.Conv1D(32, 3, padding=\"same\", dilation_rate=2)(x)\n",
    "    concat = layers.Concatenate()([conv1, conv3])\n",
    "    return models.Model(inputs, concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize function\n",
    "def normalize_data(data, mean, std):\n",
    "    return (data - mean) / std\n",
    "\n",
    "\n",
    "def ensure_even_length(sequence, padding_value=-99):\n",
    "    # If the sequence length is odd, append padding to make it even\n",
    "    if sequence.shape[0] % 2 != 0:\n",
    "        padding_shape = list(sequence.shape)\n",
    "        padding_shape[0] = 1  # we only need to add one row\n",
    "        \n",
    "        # Create a padding with the same number of columns as the sequence\n",
    "        padding = np.full(padding_shape, padding_value)\n",
    "        \n",
    "        # Append padding to the sequence\n",
    "        sequence = np.vstack([sequence, padding])\n",
    "        \n",
    "    return sequence\n",
    "\n",
    "def pad_to_max_length(sequence, max_length, padding_value=-99):\n",
    "    padding = [(0, max_length - sequence.shape[0]), (0, 0)]\n",
    "    return np.pad(sequence, padding, mode='constant', constant_values=padding_value)\n",
    "\n",
    "\n",
    "# Define a Gaussian smoothening function\n",
    "def smooth_labels(y, sigma=3):\n",
    "    return gaussian_filter1d(y, sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nbive\\AppData\\Local\\Temp\\ipykernel_15568\\2758920391.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train = np.array([df[features].values for df in train_dfs])\n",
      "C:\\Users\\nbive\\AppData\\Local\\Temp\\ipykernel_15568\\2758920391.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_train = np.array([df[\"Pick\"].values for df in train_dfs])\n",
      "C:\\Users\\nbive\\AppData\\Local\\Temp\\ipykernel_15568\\2758920391.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_val = np.array([df[features].values for df in val_dfs])\n",
      "C:\\Users\\nbive\\AppData\\Local\\Temp\\ipykernel_15568\\2758920391.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_val = np.array([df[\"Pick\"].values for df in val_dfs])\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "# Train your model here\n",
    "folder_path = current_dir + \"/Data/Well Log Data/\"\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(folder_path + file)\n",
    "    dfs.append(df)\n",
    "\n",
    "train_dfs, val_dfs = train_test_split(dfs, test_size=0.2, random_state=42) # Here 20% of the data is kept for validation\n",
    "\n",
    "features = [\"GR\", \"ILD\", \"DPHI\"]\n",
    "\n",
    "# Compute the mean and standard deviation of the training data\n",
    "all_train_data = np.vstack([df[features].values for df in train_dfs])\n",
    "mean = np.mean(all_train_data, axis=0)\n",
    "std = np.std(all_train_data, axis=0)\n",
    "\n",
    "# Extract features and labels for training and validation sets\n",
    "X_train = np.array([df[features].values for df in train_dfs])\n",
    "y_train = np.array([df[\"Pick\"].values for df in train_dfs])\n",
    "\n",
    "X_val = np.array([df[features].values for df in val_dfs])\n",
    "y_val = np.array([df[\"Pick\"].values for df in val_dfs])\n",
    "\n",
    "# # Apply smoothening to your labels\n",
    "# y_train = [np.clip(smooth_labels(y), 0, 1) for y in y_train]\n",
    "# y_val = [np.clip(smooth_labels(y), 0, 1) for y in y_val]\n",
    "\n",
    "# Apply smoothening to your labels\n",
    "y_train = [smooth_labels(y) for y in y_train]\n",
    "y_val = [smooth_labels(y) for y in y_val]\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "X_train = [normalize_data(x, mean, std) for x in X_train]\n",
    "X_val = [normalize_data(x, mean, std) for x in X_val]\n",
    "\n",
    "# Ensure even length\n",
    "X_train = [ensure_even_length(x) for x in X_train]\n",
    "X_val = [ensure_even_length(x) for x in X_val]\n",
    "\n",
    "# Compute max_length after ensuring even lengths\n",
    "max_length = max(max(x.shape[0] for x in X_train), max(x.shape[0] for x in X_val))+2\n",
    "\n",
    "# Pad to max_length\n",
    "X_train = [pad_to_max_length(x, max_length) for x in X_train]\n",
    "X_val = [pad_to_max_length(x, max_length) for x in X_val]\n",
    "\n",
    "# Ensure even length for labels and then pad\n",
    "y_train = [ensure_even_length(y.reshape(-1, 1)) for y in y_train]\n",
    "y_val = [ensure_even_length(y.reshape(-1, 1)) for y in y_val]\n",
    "\n",
    "y_train = [pad_to_max_length(y, max_length) for y in y_train]\n",
    "y_val = [pad_to_max_length(y, max_length) for y in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_train, y_train, X_val, y_val, learning_rate, epochs):\n",
    "    global_view = create_global_view()\n",
    "    local_view = create_local_view()\n",
    "\n",
    "    combined = layers.Multiply()([global_view.output, local_view.output])\n",
    "\n",
    "    # Soft Attention\n",
    "    attention_output = layers.Activation(\"tanh\")(combined)\n",
    "\n",
    "    # HYPERPARAMETER: Adjust the number of filters (1) and kernel size (1)\n",
    "    output = layers.Conv1D(1, 1, activation=\"sigmoid\")(attention_output)\n",
    "\n",
    "    model = models.Model([global_view.input, local_view.input], output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # HYPERPARAMETER: Adjust the optimizer ('adam') and its parameters\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # We'll only use this training loop and remove the redundant one.\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        epoch_train_losses = []\n",
    "        for i in range(len(X_train)):\n",
    "            X_train_well = X_train[i].reshape(1, X_train[i].shape[0], X_train[i].shape[1])\n",
    "            y_train_well = y_train[i].reshape(1, y_train[i].shape[0], 1)\n",
    "            loss = model.train_on_batch([X_train_well, X_train_well], y_train_well)  # Capturing the loss\n",
    "            epoch_train_losses.append(loss)\n",
    "        mean_train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(np.mean(mean_train_loss))\n",
    "\n",
    "        # Validation (optional)\n",
    "        epoch_val_losses = []\n",
    "        for i in range(len(X_val)):\n",
    "            X_val_well = X_val[i].reshape(1, X_val[i].shape[0], X_val[i].shape[1])\n",
    "            y_val_well = y_val[i].reshape(1, y_val[i].shape[0], 1)\n",
    "            loss = model.test_on_batch([X_val_well, X_val_well], y_val_well)\n",
    "            epoch_val_losses.append(loss)\n",
    "        mean_val_loss = np.mean(epoch_val_losses)\n",
    "        val_losses.append(mean_val_loss)\n",
    "        print(f\"Training Loss: {train_losses[-1]:.4f}, Validation Loss: {mean_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Training Loss: 0.5524, Validation Loss: 0.4215\n",
      "Epoch 2/20\n",
      "Training Loss: 0.2173, Validation Loss: -0.0920\n",
      "Epoch 3/20\n",
      "Training Loss: -0.2836, Validation Loss: -0.8316\n",
      "Epoch 4/20\n",
      "Training Loss: -1.0579, Validation Loss: -1.9379\n",
      "Epoch 5/20\n",
      "Training Loss: -2.1739, Validation Loss: -3.5574\n",
      "Epoch 6/20\n",
      "Training Loss: -3.7064, Validation Loss: -5.8951\n",
      "Epoch 7/20\n",
      "Training Loss: -5.6829, Validation Loss: -8.7592\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15568\\2092079828.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15568\\281884636.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(X_train, y_train, X_val, y_val, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mX_train_well\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0my_train_well\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train_well\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_well\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_well\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Capturing the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mepoch_train_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_train_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nbive\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2379\u001b[0m             )\n\u001b[0;32m   2380\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2381\u001b[1;33m             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2383\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nbive\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nbive\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nbive\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nbive\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nbive\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\nbive\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nbive\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_model(X_train, y_train,X_val, y_val, learning_rate = 0.00001,epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
